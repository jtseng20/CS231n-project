{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b638f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "#\n",
    "# Copyright 2018 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import argparse\n",
    "import functools\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from sg2im.data import imagenet_deprocess_batch\n",
    "#from sg2im.data.coco import CocoSceneGraphDataset, coco_collate_fn\n",
    "from sg2im.data.vg_style import VgSceneGraphDataset, vg_collate_fn\n",
    "from sg2im.discriminators import PatchDiscriminator, AcCropDiscriminator\n",
    "from sg2im.losses import get_gan_losses\n",
    "from sg2im.metrics import jaccard\n",
    "from sg2im.model import Sg2ImModel\n",
    "from sg2im.utils import int_tuple, float_tuple, str_tuple\n",
    "from sg2im.utils import timeit, bool_flag, LossManager\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1adb5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "VG_DIR = os.path.expanduser('/vision2/u/helenav/datasets/vg')\n",
    "COCO_DIR = os.path.expanduser('datasets/coco')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', default='vg', choices=['vg', 'coco'])\n",
    "\n",
    "# Optimization hyperparameters\n",
    "parser.add_argument('--batch_size', default=32, type=int)\n",
    "parser.add_argument('--num_iterations', default=160000, type=int)\n",
    "parser.add_argument('--learning_rate', default=1e-4, type=float)\n",
    "\n",
    "# Switch the generator to eval mode after this many iterations\n",
    "parser.add_argument('--eval_mode_after', default=100000, type=int)\n",
    "\n",
    "# Dataset options common to both VG and COCO\n",
    "parser.add_argument('--image_size', default='64,64', type=int_tuple)\n",
    "parser.add_argument('--num_train_samples', default=None, type=int)\n",
    "parser.add_argument('--num_val_samples', default=1024, type=int)\n",
    "parser.add_argument('--shuffle_val', default=True, type=bool_flag)\n",
    "parser.add_argument('--loader_num_workers', default=4, type=int)\n",
    "parser.add_argument('--include_relationships', default=True, type=bool_flag)\n",
    "\n",
    "# VG-specific options\n",
    "parser.add_argument('--vg_image_dir', default=os.path.join(VG_DIR, 'images'))\n",
    "parser.add_argument('--train_h5', default=os.path.join('/scr/helenav/datasets/preprocess_vg', 'stylized_train.h5'))\n",
    "parser.add_argument('--val_h5', default=os.path.join('/scr/helenav/datasets/preprocess_vg', 'stylized_val.h5'))\n",
    "parser.add_argument('--vocab_json', default=os.path.join('/scr/helenav/datasets/preprocess_vg', 'vocab.json'))\n",
    "parser.add_argument('--max_objects_per_image', default=10, type=int)\n",
    "parser.add_argument('--vg_use_orphaned_objects', default=True, type=bool_flag)\n",
    "\n",
    "# COCO-specific options\n",
    "parser.add_argument('--coco_train_image_dir',\n",
    "         default=os.path.join(COCO_DIR, 'images/train2017'))\n",
    "parser.add_argument('--coco_val_image_dir',\n",
    "         default=os.path.join(COCO_DIR, 'images/val2017'))\n",
    "parser.add_argument('--coco_train_instances_json',\n",
    "         default=os.path.join(COCO_DIR, 'annotations/instances_train2017.json'))\n",
    "parser.add_argument('--coco_train_stuff_json',\n",
    "         default=os.path.join(COCO_DIR, 'annotations/stuff_train2017.json'))\n",
    "parser.add_argument('--coco_val_instances_json',\n",
    "         default=os.path.join(COCO_DIR, 'annotations/instances_val2017.json'))\n",
    "parser.add_argument('--coco_val_stuff_json',\n",
    "         default=os.path.join(COCO_DIR, 'annotations/stuff_val2017.json'))\n",
    "parser.add_argument('--instance_whitelist', default=None, type=str_tuple)\n",
    "parser.add_argument('--stuff_whitelist', default=None, type=str_tuple)\n",
    "parser.add_argument('--coco_include_other', default=False, type=bool_flag)\n",
    "parser.add_argument('--min_object_size', default=0.02, type=float)\n",
    "parser.add_argument('--min_objects_per_image', default=3, type=int)\n",
    "parser.add_argument('--coco_stuff_only', default=True, type=bool_flag)\n",
    "\n",
    "# Generator options\n",
    "parser.add_argument('--mask_size', default=16, type=int) # Set this to 0 to use no masks\n",
    "parser.add_argument('--embedding_dim', default=128, type=int)\n",
    "parser.add_argument('--gconv_dim', default=128, type=int)\n",
    "parser.add_argument('--gconv_hidden_dim', default=512, type=int)\n",
    "parser.add_argument('--gconv_num_layers', default=5, type=int)\n",
    "parser.add_argument('--mlp_normalization', default='none', type=str)\n",
    "parser.add_argument('--refinement_network_dims', default='1024,512,256,128,64', type=int_tuple)\n",
    "parser.add_argument('--normalization', default='batch')\n",
    "parser.add_argument('--activation', default='leakyrelu-0.2')\n",
    "parser.add_argument('--layout_noise_dim', default=32, type=int)\n",
    "parser.add_argument('--use_boxes_pred_after', default=-1, type=int)\n",
    "\n",
    "# Generator losses\n",
    "parser.add_argument('--mask_loss_weight', default=0, type=float)\n",
    "parser.add_argument('--l1_pixel_loss_weight', default=1.0, type=float)\n",
    "parser.add_argument('--bbox_pred_loss_weight', default=10, type=float)\n",
    "parser.add_argument('--predicate_pred_loss_weight', default=0, type=float) # DEPRECATED\n",
    "\n",
    "# Generic discriminator options\n",
    "parser.add_argument('--discriminator_loss_weight', default=0.01, type=float)\n",
    "parser.add_argument('--gan_loss_type', default='gan')\n",
    "parser.add_argument('--d_clip', default=None, type=float)\n",
    "parser.add_argument('--d_normalization', default='batch')\n",
    "parser.add_argument('--d_padding', default='valid')\n",
    "parser.add_argument('--d_activation', default='leakyrelu-0.2')\n",
    "\n",
    "# Object discriminator\n",
    "parser.add_argument('--d_obj_arch',\n",
    "    default='C4-64-2,C4-128-2,C4-256-2')\n",
    "parser.add_argument('--crop_size', default=32, type=int)\n",
    "parser.add_argument('--d_obj_weight', default=1.0, type=float) # multiplied by d_loss_weight \n",
    "parser.add_argument('--ac_loss_weight', default=0.1, type=float)\n",
    "\n",
    "# Image discriminator\n",
    "parser.add_argument('--d_img_arch',\n",
    "    default='C4-64-2,C4-128-2,C4-256-2')\n",
    "parser.add_argument('--d_img_weight', default=1.0, type=float) # multiplied by d_loss_weight\n",
    "\n",
    "# Output options\n",
    "parser.add_argument('--print_every', default=10, type=int)\n",
    "parser.add_argument('--timing', default=False, type=bool_flag)\n",
    "parser.add_argument('--checkpoint_every', default=10000, type=int)\n",
    "parser.add_argument('--output_dir', default='/scr/helenav/checkpoints_simsg/sg2im_style/w_o_conditional_norm')\n",
    "parser.add_argument('--checkpoint_name', default='checkpoint')\n",
    "parser.add_argument('--checkpoint_start_from', default=None)\n",
    "parser.add_argument('--restore_from_checkpoint', default=False, type=bool_flag)\n",
    "\n",
    "parser.add_argument('--stylized_dir', default='/vision2/u/helenav/datasets/vg_style')\n",
    "args = parser.parse_args('')\n",
    "\n",
    "def add_loss(total_loss, curr_loss, loss_dict, loss_name, weight=1):\n",
    "  curr_loss = curr_loss * weight\n",
    "  loss_dict[loss_name] = curr_loss.item()\n",
    "  if total_loss is not None:\n",
    "    total_loss += curr_loss\n",
    "  else:\n",
    "    total_loss = curr_loss\n",
    "  return total_loss\n",
    "\n",
    "\n",
    "def check_args(args):\n",
    "  H, W = args.image_size\n",
    "  for _ in args.refinement_network_dims[1:]:\n",
    "    H = H // 2\n",
    "  if H == 0:\n",
    "    raise ValueError(\"Too many layers in refinement network\")\n",
    "\n",
    "\n",
    "def build_model(args, vocab):\n",
    "  if args.checkpoint_start_from is not None:\n",
    "    checkpoint = torch.load(args.checkpoint_start_from)\n",
    "    kwargs = checkpoint['model_kwargs']\n",
    "    model = Sg2ImModel(**kwargs)\n",
    "    raw_state_dict = checkpoint['model_state']\n",
    "    state_dict = {}\n",
    "    for k, v in raw_state_dict.items():\n",
    "      if k.startswith('module.'):\n",
    "        k = k[7:]\n",
    "      state_dict[k] = v\n",
    "    model.load_state_dict(state_dict)\n",
    "  else:\n",
    "    kwargs = {\n",
    "      'vocab': vocab,\n",
    "      'image_size': args.image_size,\n",
    "      'embedding_dim': args.embedding_dim,\n",
    "      'gconv_dim': args.gconv_dim,\n",
    "      'gconv_hidden_dim': args.gconv_hidden_dim,\n",
    "      'gconv_num_layers': args.gconv_num_layers,\n",
    "      'mlp_normalization': args.mlp_normalization,\n",
    "      'refinement_dims': args.refinement_network_dims,\n",
    "      'normalization': args.normalization,\n",
    "      'activation': args.activation,\n",
    "      'mask_size': args.mask_size,\n",
    "      'layout_noise_dim': args.layout_noise_dim,\n",
    "    }\n",
    "    model = Sg2ImModel(**kwargs)\n",
    "  return model, kwargs\n",
    "\n",
    "\n",
    "def build_obj_discriminator(args, vocab):\n",
    "  discriminator = None\n",
    "  d_kwargs = {}\n",
    "  d_weight = args.discriminator_loss_weight\n",
    "  d_obj_weight = args.d_obj_weight\n",
    "  if d_weight == 0 or d_obj_weight == 0:\n",
    "    return discriminator, d_kwargs\n",
    "\n",
    "  d_kwargs = {\n",
    "    'vocab': vocab,\n",
    "    'arch': args.d_obj_arch,\n",
    "    'normalization': args.d_normalization,\n",
    "    'activation': args.d_activation,\n",
    "    'padding': args.d_padding,\n",
    "    'object_size': args.crop_size,\n",
    "  }\n",
    "  discriminator = AcCropDiscriminator(**d_kwargs)\n",
    "  return discriminator, d_kwargs\n",
    "\n",
    "\n",
    "def build_img_discriminator(args, vocab):\n",
    "  discriminator = None\n",
    "  d_kwargs = {}\n",
    "  d_weight = args.discriminator_loss_weight\n",
    "  d_img_weight = args.d_img_weight\n",
    "  if d_weight == 0 or d_img_weight == 0:\n",
    "    return discriminator, d_kwargs\n",
    "\n",
    "  d_kwargs = {\n",
    "    'arch': args.d_img_arch,\n",
    "    'normalization': args.d_normalization,\n",
    "    'activation': args.d_activation,\n",
    "    'padding': args.d_padding,\n",
    "  }\n",
    "  discriminator = PatchDiscriminator(**d_kwargs)\n",
    "  return discriminator, d_kwargs\n",
    "\n",
    "\n",
    "def build_coco_dsets(args):\n",
    "  dset_kwargs = {\n",
    "    'image_dir': args.coco_train_image_dir,\n",
    "    'instances_json': args.coco_train_instances_json,\n",
    "    'stuff_json': args.coco_train_stuff_json,\n",
    "    'stuff_only': args.coco_stuff_only,\n",
    "    'image_size': args.image_size,\n",
    "    'mask_size': args.mask_size,\n",
    "    'max_samples': args.num_train_samples,\n",
    "    'min_object_size': args.min_object_size,\n",
    "    'min_objects_per_image': args.min_objects_per_image,\n",
    "    'instance_whitelist': args.instance_whitelist,\n",
    "    'stuff_whitelist': args.stuff_whitelist,\n",
    "    'include_other': args.coco_include_other,\n",
    "    'include_relationships': args.include_relationships,\n",
    "  }\n",
    "  train_dset = CocoSceneGraphDataset(**dset_kwargs)\n",
    "  num_objs = train_dset.total_objects()\n",
    "  num_imgs = len(train_dset)\n",
    "  print('Training dataset has %d images and %d objects' % (num_imgs, num_objs))\n",
    "  print('(%.2f objects per image)' % (float(num_objs) / num_imgs))\n",
    "\n",
    "  dset_kwargs['image_dir'] = args.coco_val_image_dir\n",
    "  dset_kwargs['instances_json'] = args.coco_val_instances_json\n",
    "  dset_kwargs['stuff_json'] = args.coco_val_stuff_json\n",
    "  dset_kwargs['max_samples'] = args.num_val_samples\n",
    "  val_dset = CocoSceneGraphDataset(**dset_kwargs)\n",
    "\n",
    "  assert train_dset.vocab == val_dset.vocab\n",
    "  vocab = json.loads(json.dumps(train_dset.vocab))\n",
    "\n",
    "  return vocab, train_dset, val_dset\n",
    "\n",
    "def build_vg_dsets(args):\n",
    "  with open(args.vocab_json, 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "  \n",
    "  dset_kwargs = {\n",
    "    'vocab': vocab,\n",
    "    'h5_path': args.train_h5,\n",
    "    'image_dir': args.vg_image_dir,\n",
    "    'image_size': args.image_size,\n",
    "    'max_samples': args.num_train_samples,\n",
    "    'max_objects': args.max_objects_per_image,\n",
    "    'use_orphaned_objects': args.vg_use_orphaned_objects,\n",
    "    'include_relationships': args.include_relationships,\n",
    "    'stylized_dir': args.stylized_dir\n",
    "  }\n",
    "  train_dset = VgSceneGraphDataset(**dset_kwargs)\n",
    "  iter_per_epoch = len(train_dset) // args.batch_size\n",
    "  print('There are %d iterations per epoch' % iter_per_epoch)\n",
    "\n",
    "  dset_kwargs['h5_path'] = args.val_h5\n",
    "  del dset_kwargs['max_samples']\n",
    "  val_dset = VgSceneGraphDataset(**dset_kwargs)\n",
    "  \n",
    "  return vocab, train_dset, val_dset\n",
    "\n",
    "\n",
    "def build_loaders(args):\n",
    "  if args.dataset == 'vg':\n",
    "    vocab, train_dset, val_dset = build_vg_dsets(args)\n",
    "    collate_fn = vg_collate_fn\n",
    "  elif args.dataset == 'coco':\n",
    "    vocab, train_dset, val_dset = build_coco_dsets(args)\n",
    "    collate_fn = coco_collate_fn\n",
    "\n",
    "  loader_kwargs = {\n",
    "    'batch_size': args.batch_size,\n",
    "    'num_workers': args.loader_num_workers,\n",
    "    'shuffle': True,\n",
    "    'collate_fn': collate_fn,\n",
    "  }\n",
    "  train_loader = DataLoader(train_dset, **loader_kwargs)\n",
    "  \n",
    "  loader_kwargs['shuffle'] = args.shuffle_val\n",
    "  val_loader = DataLoader(val_dset, **loader_kwargs)\n",
    "  return vocab, train_loader, val_loader\n",
    "\n",
    "\n",
    "def check_model(args, t, loader, model):\n",
    "  float_dtype = torch.cuda.FloatTensor\n",
    "  long_dtype = torch.cuda.LongTensor\n",
    "  num_samples = 0\n",
    "  all_losses = defaultdict(list)\n",
    "  total_iou = 0\n",
    "  total_boxes = 0\n",
    "  with torch.no_grad():\n",
    "    for batch in loader:\n",
    "      batch = [tensor.cuda() for tensor in batch]\n",
    "      masks = None\n",
    "      if len(batch) == 7:\n",
    "        imgs, style_ids, objs, boxes, triples, obj_to_img, triple_to_img = batch\n",
    "      elif len(batch) == 8:\n",
    "        imgs, style_ids, objs, boxes, masks, triples, obj_to_img, triple_to_img = batch\n",
    "      predicates = triples[:, 1] \n",
    "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "      # Run the model as it has been run during training\n",
    "      model_masks = masks\n",
    "\n",
    "      model_out = model(objs, triples, obj_to_img, boxes_gt=boxes, masks_gt=masks, style_batch=style_ids)\n",
    "\n",
    "      imgs_pred, boxes_pred, masks_pred, predicate_scores = model_out\n",
    "\n",
    "      skip_pixel_loss = False\n",
    "      total_loss, losses =  calculate_model_losses(\n",
    "                                args, skip_pixel_loss, model, imgs, imgs_pred,\n",
    "                                boxes, boxes_pred, masks, masks_pred,\n",
    "                                predicates, predicate_scores)\n",
    "\n",
    "      total_iou += jaccard(boxes_pred, boxes)\n",
    "      total_boxes += boxes_pred.size(0)\n",
    "\n",
    "      for loss_name, loss_val in losses.items():\n",
    "        all_losses[loss_name].append(loss_val)\n",
    "      num_samples += imgs.size(0)\n",
    "      if num_samples >= args.num_val_samples:\n",
    "        break\n",
    "\n",
    "    samples = {}\n",
    "    samples['gt_img'] = imgs\n",
    "\n",
    "    model_out = model(objs, triples, obj_to_img, boxes_gt=boxes, masks_gt=masks, style_batch=style_ids)\n",
    "    samples['gt_box_gt_mask'] = model_out[0]\n",
    "\n",
    "    model_out = model(objs, triples, obj_to_img, boxes_gt=boxes, style_batch=style_ids)\n",
    "    samples['gt_box_pred_mask'] = model_out[0]\n",
    "\n",
    "    model_out = model(objs, triples, obj_to_img, style_batch=style_ids)\n",
    "    samples['pred_box_pred_mask'] = model_out[0]\n",
    "\n",
    "    for k, v in samples.items():\n",
    "      samples[k] = imagenet_deprocess_batch(v)\n",
    "\n",
    "    mean_losses = {k: np.mean(v) for k, v in all_losses.items()}\n",
    "    avg_iou = total_iou / total_boxes\n",
    "\n",
    "    masks_to_store = masks\n",
    "    if masks_to_store is not None:\n",
    "      masks_to_store = masks_to_store.data.cpu().clone()\n",
    "\n",
    "    masks_pred_to_store = masks_pred\n",
    "    if masks_pred_to_store is not None:\n",
    "      masks_pred_to_store = masks_pred_to_store.data.cpu().clone()\n",
    "\n",
    "  batch_data = {\n",
    "    'style_ids': style_ids.detach().cpu().clone(),\n",
    "    'objs': objs.detach().cpu().clone(),\n",
    "    'boxes_gt': boxes.detach().cpu().clone(), \n",
    "    'masks_gt': masks_to_store,\n",
    "    'triples': triples.detach().cpu().clone(),\n",
    "    'obj_to_img': obj_to_img.detach().cpu().clone(),\n",
    "    'triple_to_img': triple_to_img.detach().cpu().clone(),\n",
    "    'boxes_pred': boxes_pred.detach().cpu().clone(),\n",
    "    'masks_pred': masks_pred_to_store\n",
    "  }\n",
    "  out = [mean_losses, samples, batch_data, avg_iou]\n",
    "\n",
    "  return tuple(out)\n",
    "\n",
    "\n",
    "def calculate_model_losses(args, skip_pixel_loss, model, img, img_pred,\n",
    "                           bbox, bbox_pred, masks, masks_pred,\n",
    "                           predicates, predicate_scores):\n",
    "  total_loss = torch.zeros(1).to(img)\n",
    "  losses = {}\n",
    "\n",
    "  l1_pixel_weight = args.l1_pixel_loss_weight\n",
    "  if skip_pixel_loss:\n",
    "    l1_pixel_weight = 0\n",
    "  l1_pixel_loss = F.l1_loss(img_pred, img)\n",
    "  total_loss = add_loss(total_loss, l1_pixel_loss, losses, 'L1_pixel_loss',\n",
    "                        l1_pixel_weight)\n",
    "  loss_bbox = F.mse_loss(bbox_pred, bbox)\n",
    "  total_loss = add_loss(total_loss, loss_bbox, losses, 'bbox_pred',\n",
    "                        args.bbox_pred_loss_weight)\n",
    "\n",
    "  if args.predicate_pred_loss_weight > 0:\n",
    "    loss_predicate = F.cross_entropy(predicate_scores, predicates)\n",
    "    total_loss = add_loss(total_loss, loss_predicate, losses, 'predicate_pred',\n",
    "                          args.predicate_pred_loss_weight)\n",
    "\n",
    "  if args.mask_loss_weight > 0 and masks is not None and masks_pred is not None:\n",
    "    mask_loss = F.binary_cross_entropy(masks_pred, masks.float())\n",
    "    total_loss = add_loss(total_loss, mask_loss, losses, 'mask_loss',\n",
    "                          args.mask_loss_weight)\n",
    "  return total_loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96f7c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "  print(args)\n",
    "  check_args(args)\n",
    "  float_dtype = torch.cuda.FloatTensor\n",
    "  long_dtype = torch.cuda.LongTensor\n",
    "\n",
    "  vocab, train_loader, val_loader = build_loaders(args)\n",
    "  model, model_kwargs = build_model(args, vocab)\n",
    "  model.type(float_dtype)\n",
    "  print(model)\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "  obj_discriminator, d_obj_kwargs = build_obj_discriminator(args, vocab)\n",
    "  img_discriminator, d_img_kwargs = build_img_discriminator(args, vocab)\n",
    "  gan_g_loss, gan_d_loss = get_gan_losses(args.gan_loss_type)\n",
    "\n",
    "  if obj_discriminator is not None:\n",
    "    obj_discriminator.type(float_dtype)\n",
    "    obj_discriminator.train()\n",
    "    print(obj_discriminator)\n",
    "    optimizer_d_obj = torch.optim.Adam(obj_discriminator.parameters(),\n",
    "                                       lr=args.learning_rate)\n",
    "\n",
    "  if img_discriminator is not None:\n",
    "    img_discriminator.type(float_dtype)\n",
    "    img_discriminator.train()\n",
    "    print(img_discriminator)\n",
    "    optimizer_d_img = torch.optim.Adam(img_discriminator.parameters(),\n",
    "                                       lr=args.learning_rate)\n",
    "\n",
    "  restore_path = None\n",
    "  if args.restore_from_checkpoint:\n",
    "    restore_path = '%s_with_model.pt' % args.checkpoint_name\n",
    "    restore_path = os.path.join(args.output_dir, restore_path)\n",
    "  if restore_path is not None and os.path.isfile(restore_path):\n",
    "    print('Restoring from checkpoint:')\n",
    "    print(restore_path)\n",
    "    checkpoint = torch.load(restore_path)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    optimizer.load_state_dict(checkpoint['optim_state'])\n",
    "\n",
    "    if obj_discriminator is not None:\n",
    "      obj_discriminator.load_state_dict(checkpoint['d_obj_state'])\n",
    "      optimizer_d_obj.load_state_dict(checkpoint['d_obj_optim_state'])\n",
    "\n",
    "    if img_discriminator is not None:\n",
    "      img_discriminator.load_state_dict(checkpoint['d_img_state'])\n",
    "      optimizer_d_img.load_state_dict(checkpoint['d_img_optim_state'])\n",
    "\n",
    "    t = checkpoint['counters']['t']\n",
    "    if 0 <= args.eval_mode_after <= t:\n",
    "      model.eval()\n",
    "    else:\n",
    "      model.train()\n",
    "    epoch = checkpoint['counters']['epoch']\n",
    "  else:\n",
    "    t, epoch = 0, 0\n",
    "    checkpoint = {\n",
    "      'args': args.__dict__,\n",
    "      'vocab': vocab,\n",
    "      'model_kwargs': model_kwargs,\n",
    "      'd_obj_kwargs': d_obj_kwargs,\n",
    "      'd_img_kwargs': d_img_kwargs,\n",
    "      'losses_ts': [],\n",
    "      'losses': defaultdict(list),\n",
    "      'd_losses': defaultdict(list),\n",
    "      'checkpoint_ts': [],\n",
    "      'train_batch_data': [], \n",
    "      'train_samples': [],\n",
    "      'train_iou': [],\n",
    "      'val_batch_data': [], \n",
    "      'val_samples': [],\n",
    "      'val_losses': defaultdict(list),\n",
    "      'val_iou': [], \n",
    "      'norm_d': [], \n",
    "      'norm_g': [],\n",
    "      'counters': {\n",
    "        't': None,\n",
    "        'epoch': None,\n",
    "      },\n",
    "      'model_state': None, 'model_best_state': None, 'optim_state': None,\n",
    "      'd_obj_state': None, 'd_obj_best_state': None, 'd_obj_optim_state': None,\n",
    "      'd_img_state': None, 'd_img_best_state': None, 'd_img_optim_state': None,\n",
    "      'best_t': [],\n",
    "    }\n",
    "\n",
    "  while True:\n",
    "    if t >= args.num_iterations:\n",
    "      break\n",
    "    epoch += 1\n",
    "    print('Starting epoch %d' % epoch)\n",
    "    \n",
    "    for batch in train_loader:\n",
    "      if t == args.eval_mode_after:\n",
    "        print('switching to eval mode')\n",
    "        model.eval()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "      t += 1\n",
    "      batch = [tensor.cuda() for tensor in batch]\n",
    "      masks = None\n",
    "      if len(batch) == 7:\n",
    "        imgs, style_ids, objs, boxes, triples, obj_to_img, triple_to_img = batch\n",
    "      elif len(batch) == 8:\n",
    "        imgs, style_ids, objs, boxes, masks, triples, obj_to_img, triple_to_img = batch\n",
    "      else:\n",
    "        assert False\n",
    "      predicates = triples[:, 1]\n",
    "      imgs = imagenet_deprocess_batch(imgs)\n",
    "      for i in range(32):\n",
    "        plt.figure()\n",
    "        plt.imshow(imgs[i].cpu().numpy().astype('uint8').transpose(1,2,0))\n",
    "\n",
    "      return\n",
    "        \n",
    "\n",
    "      with timeit('forward', args.timing):\n",
    "        model_boxes = boxes\n",
    "        model_masks = masks\n",
    "        model_out = model(objs, triples, obj_to_img,\n",
    "                          boxes_gt=model_boxes, masks_gt=model_masks, style_batch=style_ids)\n",
    "        imgs_pred, boxes_pred, masks_pred, predicate_scores = model_out\n",
    "      with timeit('loss', args.timing):\n",
    "        # Skip the pixel loss if using GT boxes\n",
    "        skip_pixel_loss = (model_boxes is None)\n",
    "        total_loss, losses =  calculate_model_losses(\n",
    "                                args, skip_pixel_loss, model, imgs, imgs_pred,\n",
    "                                boxes, boxes_pred, masks, masks_pred,\n",
    "                                predicates, predicate_scores)\n",
    "\n",
    "      if obj_discriminator is not None:\n",
    "        scores_fake, ac_loss = obj_discriminator(imgs_pred, objs, boxes, obj_to_img)\n",
    "        total_loss = add_loss(total_loss, ac_loss, losses, 'ac_loss',\n",
    "                              args.ac_loss_weight)\n",
    "        weight = args.discriminator_loss_weight * args.d_obj_weight\n",
    "        total_loss = add_loss(total_loss, gan_g_loss(scores_fake), losses,\n",
    "                              'g_gan_obj_loss', weight)\n",
    "\n",
    "      if img_discriminator is not None:\n",
    "        scores_fake = img_discriminator(imgs_pred)\n",
    "        weight = args.discriminator_loss_weight * args.d_img_weight\n",
    "        total_loss = add_loss(total_loss, gan_g_loss(scores_fake), losses,\n",
    "                              'g_gan_img_loss', weight)\n",
    "\n",
    "      losses['total_loss'] = total_loss.item()\n",
    "      if not math.isfinite(losses['total_loss']):\n",
    "        print('WARNING: Got loss = NaN, not backpropping')\n",
    "        continue\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      with timeit('backward', args.timing):\n",
    "        total_loss.backward()\n",
    "      optimizer.step()\n",
    "      total_loss_d = None\n",
    "      ac_loss_real = None\n",
    "      ac_loss_fake = None\n",
    "      d_losses = {}\n",
    "      \n",
    "      if obj_discriminator is not None:\n",
    "        d_obj_losses = LossManager()\n",
    "        imgs_fake = imgs_pred.detach()\n",
    "        scores_fake, ac_loss_fake = obj_discriminator(imgs_fake, objs, boxes, obj_to_img)\n",
    "        scores_real, ac_loss_real = obj_discriminator(imgs, objs, boxes, obj_to_img)\n",
    "\n",
    "        d_obj_gan_loss = gan_d_loss(scores_real, scores_fake)\n",
    "        d_obj_losses.add_loss(d_obj_gan_loss, 'd_obj_gan_loss')\n",
    "        d_obj_losses.add_loss(ac_loss_real, 'd_ac_loss_real')\n",
    "        d_obj_losses.add_loss(ac_loss_fake, 'd_ac_loss_fake')\n",
    "\n",
    "        optimizer_d_obj.zero_grad()\n",
    "        d_obj_losses.total_loss.backward()\n",
    "        optimizer_d_obj.step()\n",
    "\n",
    "      if img_discriminator is not None:\n",
    "        d_img_losses = LossManager()\n",
    "        imgs_fake = imgs_pred.detach()\n",
    "        scores_fake = img_discriminator(imgs_fake)\n",
    "        scores_real = img_discriminator(imgs)\n",
    "\n",
    "        d_img_gan_loss = gan_d_loss(scores_real, scores_fake)\n",
    "        d_img_losses.add_loss(d_img_gan_loss, 'd_img_gan_loss')\n",
    "        \n",
    "        optimizer_d_img.zero_grad()\n",
    "        d_img_losses.total_loss.backward()\n",
    "        optimizer_d_img.step()\n",
    "\n",
    "      if t % args.print_every == 0:\n",
    "        print('t = %d / %d' % (t, args.num_iterations))\n",
    "        print(args.output_dir)\n",
    "        for name, val in losses.items():\n",
    "          print(' G [%s]: %.4f' % (name, val))\n",
    "          checkpoint['losses'][name].append(val)\n",
    "        checkpoint['losses_ts'].append(t)\n",
    "\n",
    "        if obj_discriminator is not None:\n",
    "          for name, val in d_obj_losses.items():\n",
    "            print(' D_obj [%s]: %.4f' % (name, val))\n",
    "            checkpoint['d_losses'][name].append(val)\n",
    "\n",
    "        if img_discriminator is not None:\n",
    "          for name, val in d_img_losses.items():\n",
    "            print(' D_img [%s]: %.4f' % (name, val))\n",
    "            checkpoint['d_losses'][name].append(val)\n",
    "      \n",
    "      if t % args.checkpoint_every == 0:\n",
    "        print('checking on train')\n",
    "        train_results = check_model(args, t, train_loader, model)\n",
    "        t_losses, t_samples, t_batch_data, t_avg_iou = train_results\n",
    "\n",
    "        checkpoint['train_batch_data'].append(t_batch_data)\n",
    "        checkpoint['train_samples'].append(t_samples)\n",
    "        checkpoint['checkpoint_ts'].append(t)\n",
    "        checkpoint['train_iou'].append(t_avg_iou)\n",
    "\n",
    "        print('checking on val')\n",
    "        val_results = check_model(args, t, val_loader, model)\n",
    "        val_losses, val_samples, val_batch_data, val_avg_iou = val_results\n",
    "        checkpoint['val_samples'].append(val_samples)\n",
    "        checkpoint['val_batch_data'].append(val_batch_data)\n",
    "        checkpoint['val_iou'].append(val_avg_iou)\n",
    "\n",
    "        print('train iou: ', t_avg_iou)\n",
    "        print('val iou: ', val_avg_iou)\n",
    "\n",
    "        for k, v in val_losses.items():\n",
    "          checkpoint['val_losses'][k].append(v)\n",
    "        checkpoint['model_state'] = model.state_dict()\n",
    "\n",
    "        if obj_discriminator is not None:\n",
    "          checkpoint['d_obj_state'] = obj_discriminator.state_dict()\n",
    "          checkpoint['d_obj_optim_state'] = optimizer_d_obj.state_dict()\n",
    "\n",
    "        if img_discriminator is not None:\n",
    "          checkpoint['d_img_state'] = img_discriminator.state_dict()\n",
    "          checkpoint['d_img_optim_state'] = optimizer_d_img.state_dict()\n",
    "\n",
    "        checkpoint['optim_state'] = optimizer.state_dict()\n",
    "        checkpoint['counters']['t'] = t\n",
    "        checkpoint['counters']['epoch'] = epoch\n",
    "        checkpoint_path = os.path.join(args.output_dir,\n",
    "                              '%s_with_model.pt' % args.checkpoint_name)\n",
    "        print('Saving checkpoint to ', checkpoint_path)\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "        # Save another checkpoint without any model or optim state\n",
    "        checkpoint_path = os.path.join(args.output_dir,\n",
    "                              '%s_no_model.pt' % args.checkpoint_name)\n",
    "        key_blacklist = ['model_state', 'optim_state', 'model_best_state',\n",
    "                         'd_obj_state', 'd_obj_optim_state', 'd_obj_best_state',\n",
    "                         'd_img_state', 'd_img_optim_state', 'd_img_best_state']\n",
    "        small_checkpoint = {}\n",
    "        for k, v in checkpoint.items():\n",
    "          if k not in key_blacklist:\n",
    "            small_checkpoint[k] = v\n",
    "        torch.save(small_checkpoint, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceb8000d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(ac_loss_weight=0.1, activation='leakyrelu-0.2', batch_size=32, bbox_pred_loss_weight=10, checkpoint_every=10000, checkpoint_name='checkpoint', checkpoint_start_from=None, coco_include_other=False, coco_stuff_only=True, coco_train_image_dir='datasets/coco/images/train2017', coco_train_instances_json='datasets/coco/annotations/instances_train2017.json', coco_train_stuff_json='datasets/coco/annotations/stuff_train2017.json', coco_val_image_dir='datasets/coco/images/val2017', coco_val_instances_json='datasets/coco/annotations/instances_val2017.json', coco_val_stuff_json='datasets/coco/annotations/stuff_val2017.json', crop_size=32, d_activation='leakyrelu-0.2', d_clip=None, d_img_arch='C4-64-2,C4-128-2,C4-256-2', d_img_weight=1.0, d_normalization='batch', d_obj_arch='C4-64-2,C4-128-2,C4-256-2', d_obj_weight=1.0, d_padding='valid', dataset='vg', discriminator_loss_weight=0.01, embedding_dim=128, eval_mode_after=100000, gan_loss_type='gan', gconv_dim=128, gconv_hidden_dim=512, gconv_num_layers=5, image_size=(64, 64), include_relationships=True, instance_whitelist=None, l1_pixel_loss_weight=1.0, layout_noise_dim=32, learning_rate=0.0001, loader_num_workers=4, mask_loss_weight=0, mask_size=16, max_objects_per_image=10, min_object_size=0.02, min_objects_per_image=3, mlp_normalization='none', normalization='batch', num_iterations=160000, num_train_samples=None, num_val_samples=1024, output_dir='/scr/helenav/checkpoints_simsg/sg2im_style/w_o_conditional_norm', predicate_pred_loss_weight=0, print_every=10, refinement_network_dims=(1024, 512, 256, 128, 64), restore_from_checkpoint=False, shuffle_val=True, stuff_whitelist=None, stylized_dir='/vision2/u/helenav/datasets/vg_style', timing=False, train_h5='/scr/helenav/datasets/preprocess_vg/stylized_train.h5', use_boxes_pred_after=-1, val_h5='/scr/helenav/datasets/preprocess_vg/stylized_val.h5', vg_image_dir='/vision2/u/helenav/datasets/vg/images', vg_use_orphaned_objects=True, vocab_json='/scr/helenav/datasets/preprocess_vg/vocab.json')\n",
      "There are 3780 iterations per epoch\n",
      "Sg2ImModel(\n",
      "  (obj_embeddings): Embedding(180, 128)\n",
      "  (pred_embeddings): Embedding(46, 128)\n",
      "  (gconv): GraphTripleConv(\n",
      "    (net1): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1152, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (net2): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (gconv_net): GraphTripleConvNet(\n",
      "    (gconvs): ModuleList(\n",
      "      (0): GraphTripleConv(\n",
      "        (net1): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=1152, bias=True)\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (net2): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (3): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (1): GraphTripleConv(\n",
      "        (net1): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=1152, bias=True)\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (net2): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (3): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (2): GraphTripleConv(\n",
      "        (net1): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=1152, bias=True)\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (net2): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (3): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (3): GraphTripleConv(\n",
      "        (net1): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=1152, bias=True)\n",
      "          (3): ReLU()\n",
      "        )\n",
      "        (net2): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (3): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (box_net): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mask_net): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU()\n",
      "    (16): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (rel_aux_net): Sequential(\n",
      "    (0): Linear(in_features=264, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=46, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (refinement_net): RefinementNetwork(\n",
      "    (refinement_modules): ModuleList(\n",
      "      (0): RefinementModule(\n",
      "        (net): Sequential(\n",
      "          (0): Conv2d(162, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.2)\n",
      "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): LeakyReLU(negative_slope=0.2)\n",
      "        )\n",
      "      )\n",
      "      (1): RefinementModule(\n",
      "        (net): Sequential(\n",
      "          (0): Conv2d(1184, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.2)\n",
      "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): LeakyReLU(negative_slope=0.2)\n",
      "        )\n",
      "      )\n",
      "      (2): RefinementModule(\n",
      "        (net): Sequential(\n",
      "          (0): Conv2d(672, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.2)\n",
      "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): LeakyReLU(negative_slope=0.2)\n",
      "        )\n",
      "      )\n",
      "      (3): RefinementModule(\n",
      "        (net): Sequential(\n",
      "          (0): Conv2d(416, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.2)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): LeakyReLU(negative_slope=0.2)\n",
      "        )\n",
      "      )\n",
      "      (4): RefinementModule(\n",
      "        (net): Sequential(\n",
      "          (0): Conv2d(288, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): LeakyReLU(negative_slope=0.2)\n",
      "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): LeakyReLU(negative_slope=0.2)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_conv): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.2)\n",
      "      (2): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "LeakyReLU(negative_slope=0.2)\n",
      "Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "LeakyReLU(negative_slope=0.2)\n",
      "Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
      "Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "LeakyReLU(negative_slope=0.2)\n",
      "Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
      "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "LeakyReLU(negative_slope=0.2)\n",
      "Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
      "AcCropDiscriminator(\n",
      "  (discriminator): AcDiscriminator(\n",
      "    (cnn): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.2)\n",
      "        (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.2)\n",
      "        (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
      "      )\n",
      "      (1): GlobalAvgPool()\n",
      "      (2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    )\n",
      "    (real_classifier): Linear(in_features=1024, out_features=1, bias=True)\n",
      "    (obj_classifier): Linear(in_features=1024, out_features=179, bias=True)\n",
      "  )\n",
      ")\n",
      "PatchDiscriminator(\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2)\n",
      "    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2)\n",
      "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
      "  )\n",
      "  (classifier): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10a9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a183c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
